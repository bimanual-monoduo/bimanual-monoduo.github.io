<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MonoDuo: Using One Robot Arm to Learn Bimanual Policies">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MonoDuo: Using One Robot Arm to Learn Bimanual Policies</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="image/x-icon" href="static/images/unnamed.ico">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-979K1TQ2B8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-979K1TQ2B8');
</script>

<body>
  <!-- Intro -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MonoDuo: Using One Robot Arm to Learn Bimanual Policies</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://sunnythecode.github.io/">Sandeep Bajamahal*</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://yunliangchen.github.io/">Lawrence Yunliang Chen*</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://toruowo.github.io/">Toru Lin</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="">Zehan Ma</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a><sup></sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>UC Berkeley</span>

              <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>* Equal Contribution</sup></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- Dataset Link. -->

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- 5 task fig & desc -->
  <section class="hero teaser" style="margin-bottom: 0; padding-bottom: 1rem;">
    <div class="container is-max-desktop">
      <div class="hero-body p-4">
        <img src="./static/images/fig1v5.png" class="interpolation-image" alt="MonoDuo 5-task image" />
        <h2 class="subtitle has-text-centered">
          <strong>MonoDuo</strong> trains bimanual policies from collaborative demonstrations of human and robot arms.
        </h2>
      </div>
    </div>
  </section>

  <!-- Abstract  -->
  <section>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Bimanual coordination is essential for many real-world manipulation tasks, yet learning bimanual robot
              policies is
              limited by the scarcity of bimanual robots and datasets. Single-arm robots, however, are widely available
              in
              research labs. <strong>Can we leverage them to train bimanual robot policies?</strong>
            </p>
            <p>
              We present <strong>MonoDuo</strong>, a framework
              for <strong>learning bimanual manipulation policies using single-arm robot demonstrations</strong> paired
              with human
              collaboration. MonoDuo collects data by teleoperating a single-arm robot to perform one side of a bimanual
              task
              while a human performs the other, then swapping roles to cover both sides. RGB-D
              observations from a wrist-mounted
              and fixed camera are augmented into synthetic demonstrations for target bimanual robots
              using state-of-the-art hand pose
              estimation, image and point cloud segmentation, and
              inpainting. These synthetic demonstrations, grounded in real robot kinematics,
              are used to train bimanual policies.
            </p>
            <p>
              We evaluate MonoDuo on <strong>five tasks</strong>â€”box lifting, backpack packing, cloth folding, jacket
              zipping, and plate handover. Compared to approaches relying solely on human bimanual videos, MonoDuo
              enables zero-shot deployment on
              unseen bimanual robot configurations, achieving success rates up to <strong>70%</strong>. With only
              25 target robot demonstrations, few-shot finetuning further boosts
              success rates by <strong>65-70%</strong> over training from scratch, demonstrating MonoDuo's effectiveness
              in efficiently transferring
              knowledge from single-arm robot data to bimanual robot policies.
            </p>

          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <!-- Overview -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Title -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Overview</h2>
      </div>
    </div>

    <!-- Image block (full-width) -->
    <div class="container has-text-centered">
      <img src="./static/images/fig2v11.png" class="overview-image" alt="MonoDuo 5-task image" />
    </div>

    <!-- Text block -->
    <div class="container is-max-desktop"> <!-- add spacing -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              Our teleoperation system uses a fixed RGB-D camera and a wrist-mounted camera. We teleoperate a single-arm
              robot to
              collaborate with a human arm on a bimanual task, alternating
              left-right arm roles across episodes.
              This results in complementary interaction data covering both sides of the task.
              These human-robot bimanual demonstrations are then
              augmented into synthetic robot-robot bimanual demonstrations
              to
              create a visually and
              physically grounded dataset for training bimanual robots.
            </p>
            <p>
              Below are samples of collected demonstration data for five tasks:
              <b>Fold Cloth</b>, <b>Plate Handover</b>, <b>Zip Jacket</b>, <b>Lift Box</b>, and <b>Pack Back</b>.

            </p>
          </div>
        </div>
      </div>
    </div>

  </section>


  <!-- Human-Robot Demo Carousel -->
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div class="has-text-centered mb-4">
          <h2 class="title is-4"> Demonstration Examples </h2>
          <p class="subtitle is-6">Examples of our human-robot data collection across various tasks.</p>
        </div>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/bagorig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/foldingorig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/handoverorig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ziporig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/boxorig2.mp4" type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </section>



  <!-- Pipeline -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Title -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">MonoDuo Pipeline</h2>
      </div>
    </div>

    <!-- Image block (full-width) -->
    <div class="container has-text-centered">
      <img src="./static/images/fig3v4.png" class="overview-image" alt="MonoDuo 5-task image" />
    </div>

    <!-- Text block -->
    <div class="container is-max-desktop"> <!-- add spacing -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              <b><i>Left:</i></b> We apply <a href="https://geopavlakos.github.io/hamer/" target="_blank">HaMeR</a>
              to estimate the hand pose at each frame and
              refine with ICP. The refined hand pose is then retargeted
              into robot end-effector actions in the source dataset.
              <b><i>Right:</i></b> We perform cross-painting from both the source robot and the
              human arm to the target robot.
            </p>
            <p>
              Rollout videos on policy are shown below.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Rollout videos -->


  <!-- Results -->
  <section>
    <div class="container is-max-desktop">
      <!-- Title -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Results</h2>
      </div>
    </div>
    <br>
    <section class="hero is-light is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-steve">
              <video id="steve" autoplay controls muted loop playsinline width="1080" height="720"
                style="object-fit: fill;">
                <source src="./static/videos/bag_rollout_1080x720_web.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-chair-tp">
              <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/box_rollout_1080x1080-9252_1080x720_web.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-fullbody">
              <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/folding_1080x1080-10391_1080x720_web.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-blueshirt">
              <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/handover_rollout_1080x1080_2x-10559_1080x720_web.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-mask">
              <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/zipping_rollout_1080x1080_2x-9231_1080x720_web.mp4" type="video/mp4">
              </video>
            </div>

          </div>
        </div>
      </div>
    </section>

    <br>



  </section>



  <section>
    <div class="container is-max-desktop">
      <!-- Text block -->
      <div class="container is-max-desktop"> <!-- add spacing -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <p>
                Each policy is evaluated on five manipulation tasks in a zero-shot
                transfer setting from Franka-human demos to a bimanual UR5e. We collect and train on 200 Franka-human
                demos per task.
                The results are shown below. The following show that MonoDuo is able to effectively bridge
                both the visual and physical gaps among different robots and
                human, allowing one to learn bimanual policies when only a
                single-arm robot is available.
              </p>
            </div>
          </div>
        </div>
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-4">Quantitative Performance Summary</h3>
              <div class="content has-text-justified">
                <p>
                  MonoDuo achieves strong zero-shot bimanual manipulation performance despite training only from
                  single-arm robot demonstrations.
                  Across all five tasks, MonoDuo outperforms baselines that rely on human-only bimanual video training.
                  The gains are most pronounced in tasks requiring complementary role coordination (e.g., <b>zip
                    jacket</b> and <b>pack backpack</b>), highlighting the value of robot-grounded interaction data.
                </p>
                <p>
                  Additionally, when limited target-robot demonstrations (25 per task) are available, few-shot
                  finetuning dramatically improves success rates.
                  This shows that MonoDuo is not only effective for zero-shot transfer, but also provides a strong
                  initialization for rapid adaptation to new bimanual setups.
                </p>
              </div>
            </div>
          </div>
        </div>

      </div>
      <div class="container has-text-centered">
        <img src="./static/images/table1.png" class="col-image" alt="MonoDuo 5-task image" />

      </div>

      <div class="columns is-centered">

        <!-- Visual Effects. -->
        <div class="column">
            <h2 class="title is-3">Impact of Wrist-Camera</h2>
            <img src="./static/images/table2.png" class="left-image" alt="MonoDuo 5-task image" />
        </div>
        <!--/ Visual Effects. -->

        <!-- Matting. -->
        <div class="column has-text-left">
          <h2 class="title is-3">Few-Shot Learning</h2>
          <img src="./static/images/table3.png" class="left-image" alt="MonoDuo 5-task image" />
        </div>
      </div>
      <!--/ Matting. -->




    </div>
  </section>




  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!-- <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a> -->
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <!-- <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p> -->
            <p>
              Template code is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies
              </a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>