<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MonoDuo: Using One Robot Arm to Learn Bimanual Policies">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MonoDuo: Using One Robot Arm to Learn Bimanual Policies</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- Intro -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MonoDuo: Using One Robot Arm to Learn Bimanual Policies</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://sunnythecode.github.io/">Sandeep Bajamahal*</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://yunliangchen.github.io/">Lawrence Yunliang Chen*</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://toruowo.github.io/">Toru Lin</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="">Zehan Ma</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a><sup></sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>UC Berkeley</span>

              <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>* Equal Contribution</sup></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- Dataset Link. -->
                  
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- 5 task fig & desc -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body p-4">
        <img src="./static/images/fig1v5.png" class="interpolation-image" alt="MonoDuo 5-task image" />
        <h2 class="subtitle has-text-centered">
          <strong>MonoDuo</strong> trains bimanual policies from collaborative demonstrations of human and robot arms.
        </h2>
      </div>
    </div>
  </section>

  <!-- Abstract  -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Bimanual coordination is essential for many real-world manipulation tasks, yet learning bimanual robot
              policies is
              limited by the scarcity of bimanual robots and datasets. Single-arm robots, however, are widely available
              in
              research labs. <strong>Can we leverage them to train bimanual robot policies?</strong>
            </p>
            <p>
              We present <strong>MonoDuo</strong>, a framework
              for <strong>learning bimanual manipulation policies using single-arm robot demonstrations</strong> paired
              with human
              collaboration. MonoDuo collects data by teleoperating a single-arm robot to perform one side of a bimanual
              task
              while a human performs the other, then swapping roles to cover both sides. RGB-D
              observations from a wrist-mounted
              and fixed camera are augmented into synthetic demonstrations for target bimanual robots
              using state-of-the-art hand pose
              estimation, image and point cloud segmentation, and
              inpainting. These synthetic demonstrations, grounded in real robot kinematics,
              are used to train bimanual policies.
            </p>
            <p>
              We evaluate MonoDuo on <strong>five tasks</strong>â€”box lifting, backpack packing, cloth folding, jacket
              zipping, and plate handover. Compared to approaches relying solely on human bimanual videos, MonoDuo
              enables zero-shot deployment on
              unseen bimanual robot configurations, achieving success rates up to <strong>70%</strong>. With only
              25 target robot demonstrations, few-shot finetuning further boosts
              success rates by <strong>65-70%</strong> over training from scratch, demonstrating MonoDuo's effectiveness
              in efficiently transferring
              knowledge from single-arm robot data to bimanual robot policies.
            </p>

          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <!-- Overview -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Title -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Overview</h2>
      </div>
    </div>

    <!-- Image block (full-width) -->
    <div class="container has-text-centered">
      <img src="./static/images/fig2v11.png" class="overview-image" alt="MonoDuo 5-task image" />
    </div>

    <!-- Text block -->
    <div class="container is-max-desktop"> <!-- add spacing -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              Our teleoperation system uses a fixed RGB-D camera and a wrist-mounted camera. We teleoperate a single-arm
              robot to
              collaborate with a human arm on a bimanual task, alternating
              left-right arm roles across episodes.
              This results in complementary interaction data covering both sides of the task.
              These human-robot bimanual demonstrations are then
              augmented into synthetic robot-robot bimanual demonstrations
              to
              create a visually and
              physically grounded dataset for training bimanual robots.
            </p>
            <p>
              Below are samples of collected demonstration data for five tasks:
              <b>Fold Cloth</b>, <b>Plate Handover</b>, <b>Zip Jacket</b>, <b>Lift Box</b>, and <b>Pack Back</b>.

            </p>
          </div>
        </div>
      </div>
    </div>

  </section>


  <!-- Human-Robot Demo Carousel -->
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/bagorig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/foldingorig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/handoverorig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ziporig2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/boxorig2.mp4" type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </section>



  <!-- Pipeline -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Title -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">MonoDuo Pipeline</h2>
      </div>
    </div>

    <!-- Image block (full-width) -->
    <div class="container has-text-centered">
      <img src="./static/images/fig3v4.png" class="overview-image" alt="MonoDuo 5-task image" />
    </div>

    <!-- Text block -->
    <div class="container is-max-desktop"> <!-- add spacing -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              <b><i>Left:</i></b> We apply <a href="https://geopavlakos.github.io/hamer/" target="_blank">HaMeR</a>
              to estimate the hand pose at each frame and
              refine with ICP. The refined hand pose is then retargeted
              into robot end-effector actions in the source dataset.
              <b><i>Right:</i></b> We perform cross-painting from both the source robot and the
              human arm to the target robot.
            </p>
            <p>
              Rollout videos on policy are shown below. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video id="steve" autoplay controls muted loop playsinline width="1080" height="720" style="object-fit: fill;">
              <source src="./static/videos/bag_rollout_1080x720_web.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/box_rollout_1080x1080-9252_1080x720_web.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/folding_1080x1080-10391_1080x720_web.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/handover_rollout_1080x1080_2x-10559_1080x720_web.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/zipping_rollout_1080x1080_2x-9231_1080x720_web.mp4" type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">

        <!-- Visual Effects. -->
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Results</h2>
            <p>
              Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
              would be impossible without nerfies since it would require going through a wall.
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/dollyzoom-stacked.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Visual Effects. -->

        <!-- Matting. -->
        <div class="column">
          <h2 class="title is-3">Matting</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p>
              <video id="matting-video" controls playsinline height="100%">
                <source src="./static/videos/matting.mp4" type="video/mp4">
              </video>
            </div>

          </div>
        </div>
      </div>
      <!--/ Matting. -->

      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Animation</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Interpolating states</h3>
          <div class="content has-text-justified">
            <p>
              We can also animate the scene by interpolating the deformation latent codes of two input
              frames. Use the slider here to linearly interpolate between the left frame and the right
              frame.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-3 has-text-centered">
              <img src="./static/images/interpolate_start.jpg" class="interpolation-image"
                alt="Interpolate start reference image." />
              <p>Start Frame</p>
            </div>
            <div class="column interpolation-video-column">
              <div id="interpolation-image-wrapper">
                Loading...
              </div>
              <input class="slider is-fullwidth is-large is-info" id="interpolation-slider" step="1" min="0" max="100"
                value="0" type="range">
            </div>
            <div class="column is-3 has-text-centered">
              <img src="./static/images/interpolate_end.jpg" class="interpolation-image"
                alt="Interpolation end reference image." />
              <p class="is-bold">End Frame</p>
            </div>
          </div>
          <br />
          <!--/ Interpolating. -->

          <!-- Re-rendering. -->
          <h3 class="title is-4">Re-rendering the input video</h3>
          <div class="content has-text-justified">
            <p>
              Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
              viewpoint such as a stabilized camera by playing back the training deformations.
            </p>
          </div>
          <div class="content has-text-centered">
            <video id="replay-video" controls muted preload playsinline width="75%">
              <source src="./static/videos/replay.mp4" type="video/mp4">
            </video>
          </div>
          <!--/ Re-rendering. -->

        </div>
      </div>
      <!--/ Animation. -->


      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>

          <div class="content has-text-justified">
            <p>
              There's a lot of excellent work that was introduced around the same time as ours.
            </p>
            <p>
              <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an
              idea similar to our windowed position encoding for coarse-to-fine optimization.
            </p>
            <p>
              <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a
                href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
              both use deformation fields to model non-rigid scenes.
            </p>
            <p>
              Some works model videos with a NeRF by directly modulating the density, such as <a
                href="https://video-nerf.github.io/">Video-NeRF</a>, <a
                href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a
                href="https://neural-3d-video.github.io/">DyNeRF</a>
            </p>
            <p>
              There are probably many more by the time you are reading this. Check out <a
                href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a
                href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
            </p>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Template code from <a href="https://github.com/nerfies/nerfies.github.io">here
                </a> 
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>